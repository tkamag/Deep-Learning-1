{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras I (intro)\n",
    "- Part II: [Deep Learning w Keras II (mnist data, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20II%20(mnist%20data%2C%20mlp).ipynb)\n",
    "- Part III: [Deep Learning w Keras III (student admissions, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20III%20(student%20admissions%2C%20mlp)%20.ipynb)\n",
    "- Part IV: [Deep Learning w Keras IV (imdb, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20IV%20(imdb%2C%20mlp).ipynb)\n",
    "\n",
    "\n",
    "\n",
    "Some ressources:\n",
    "- Deep learning and neural networks:\n",
    " - [Neural network playlist (3Blue1Brown)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    " - [Book: Deep Learning (Goodfellow, Bengio, Courville)](https://www.deeplearningbook.org/) or as [pdf](https://github.com/janishar/mit-deep-learning-book-pdf)\n",
    "- Keras\n",
    " - [Keras examples](https://github.com/keras-team/keras/tree/master/examples)\n",
    " - [Getting Started with Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/)\n",
    " \n",
    "More links at the relevant point in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook demonstrates the use of the Deep Learning framework **Keras**, followed by an example of how to apply this to a dataset.\n",
    "\n",
    "### `Sequential` model: \n",
    "- [Sequential](https://keras.io/models/sequential/) \n",
    "- Linear stack of layers which is used to create a sequential model by passing a list of layer instances. \n",
    "\n",
    "Example: \n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "]) \n",
    "```\n",
    "Here, we define a model with 784 inputs $x_1, x_2, ..., x_{784}$, leading to a hidden layer of 32 nodes and `relu`activation function. The output layer consists of 10 nodes (e.g. for ten categories) with a `softmax` activation function. The input shape of the first layer has to be specified but the following layer adapts to the previous one. Then again, the output shape has to be specified. An equivalent way to specify the model is to use the `.add()` method, where layers are added to the model using `model.add(Dense(32, input_dim = 784))` and so forth. \n",
    "\n",
    "### Compilation using `.compile()`\n",
    "Before training a model, we need to configure how the model is supposed to learn the function. Therefore, three arguments are required, namely `optimizer`, `loss function`, and `metric`: \n",
    "1. The `optimizer` (eg. SGD, RMSprop, Adagrad)\n",
    " - Optimization algorithms are necessary to minimize (respectively maximize) our objective function (loss function) and to learn the parameters of the model. \n",
    " -  [Keras optimizers](https://keras.io/optimizers)\n",
    " - [How to train NN faster with optimizers?](https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713)\n",
    " - [Types of Optimization Algorithms used in NN ...](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)\n",
    " \n",
    "```python\n",
    "# Import optimizers\n",
    "from keras import optimizers\n",
    "\n",
    "# specify keras model\n",
    "model = ...\n",
    "\n",
    "# define SGD optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = sgd)\n",
    "```\n",
    "\n",
    "2. A `loss function` is the function we want to minimize or maximize. It is used to evaluate a candidate solution (e.g. the current weights of the model). The optimizer minimizes or maximizes the loss of this function by tweaking the model parameters, leading finally to the parameters that are associated with the optimal loss. It is important that the chosen loss function represents our designated goals.      \n",
    " - [Keras loss functions](https://keras.io/losses)\n",
    " - [ML-Mastery: Loss and Loss Functions](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
    "\n",
    "```python\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")\n",
    "\n",
    "from keras import losses\n",
    "model.compile(loss = losses.mean_squared_error, optimizer = \"sgd\")\n",
    "```\n",
    "\n",
    "3. A `metric` to judge the performance of the model. The loss function reduces all good/bad aspects of the complex system down to a single number. Unfortunately, this number doesn't tell us much about the actual performance of the model. Hence, we'd like to include a measure that tells us more about how good the model actually is. \n",
    " - [Keras metrics](https://keras.io/metrics/)\n",
    "\n",
    "#### Examples:`.compile()` \n",
    "```python\n",
    "# Multi-class classification problem\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# Binary classification problem\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# Mean squared error regression problem\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"mse\")\n",
    "    \n",
    "# Custom metrics #\n",
    "# Import\n",
    "import keras.backend as K\n",
    "\n",
    "# Define own metric \n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = \"sgd\",\n",
    "              metrics = [\"accuracy\", mean_pred])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Training model with `.fit()`\n",
    "The last step is to fit the model to the data. Keras models use Numpy arrays of input data and labels (outcome variable y). In general, the `.fit()` function is used. This function trains the model for a given number of epochs (iterations on dataset). The documentation can be found [here](https://keras.io/models/sequential/). It is very flexible but its main arguments are:\n",
    "- `x:` to specify training data\n",
    "- `y:` to specify target data\n",
    "- `batch_size:` Number of samples per gradient update. Default is 32. \n",
    "- `epochs:` Number of epochs to train the model. One epoch is one iteration over the entire x and y data.\n",
    "- `shuffle:` Boolean to indicate whether to shuffle training data before each epoch (required if data are of some sort of order).\n",
    "- Other arguments for more specific cases are available such as changing the verbosity mode, including validation data, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Model I: Binary classification with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 266us/step - loss: 0.7035 - acc: 0.5190\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.6951 - acc: 0.5340\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.6940 - acc: 0.5380\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.6902 - acc: 0.5410\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.6846 - acc: 0.5460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b62fb3a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim = 100),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(1),\n",
    "    Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=\"rmsprop\", \n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# Generate dummy data\n",
    "data = np.random.random((1000, 100))\n",
    "y = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, y, epochs = 5, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Model II: Categorical classification with dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 204us/step - loss: 2.3660 - acc: 0.0960 - mean_squared_error: 0.0912\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.3157 - acc: 0.1160 - mean_squared_error: 0.0903\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3064 - acc: 0.1230 - mean_squared_error: 0.0901\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.2972 - acc: 0.1300 - mean_squared_error: 0.0899\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 2.2905 - acc: 0.1320 - mean_squared_error: 0.0898\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.2846 - acc: 0.1350 - mean_squared_error: 0.0897\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.2797 - acc: 0.1420 - mean_squared_error: 0.0895\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 2.2713 - acc: 0.1530 - mean_squared_error: 0.0894\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.2671 - acc: 0.1570 - mean_squared_error: 0.0893\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.2627 - acc: 0.1650 - mean_squared_error: 0.0892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b642e0eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential([\n",
    "    Dense(32, activation = \"relu\", input_dim = 100),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics = [\"accuracy\", \"mse\"])\n",
    "\n",
    "# Dummy data\n",
    "data = np.random.random((1000, 100))\n",
    "y = np.random.randint(10, size = (1000, 1))\n",
    "\n",
    "# y to categorical matrix\n",
    "cat_y = utils.to_categorical(y, num_classes = 10)\n",
    "\n",
    "# Fitting the model to data\n",
    "model.fit(data, cat_y, epochs=10, batch_size=64, shuffle = True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Both models fit the model to the data and reaching a certain accuracy if we'd allow the model to run for many epochs. For example for around 100 epochs we reach an accuracy of more than 50%. \n",
    "\n",
    "However, this is due to the fact that the model \"memorizes\" these random data. Hence, the model may fit the training data well but this is in no way generalizable as we are dealing with random data. \n",
    "\n",
    "That's why it is important to include test data and cross-validation to ensure that we assess the fit of the model which the model did not see before (i.e. data that were not used for training the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The End ###\n"
     ]
    }
   ],
   "source": [
    "print(\"### The End ###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
