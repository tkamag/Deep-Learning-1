{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras II (mnist data, mlp)\n",
    "- Part I: [Deep Learning w Keras I (Intro)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20I%20(intro).ipynb)\n",
    "- Part III: [Deep Learning w Keras III (student admissions, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20III%20(student%20admissions%2C%20mlp)%20.ipynb)\n",
    "- Part IV: [Deep Learning w Keras IV (imdb, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20IV%20(imdb%2C%20mlp).ipynb)\n",
    "\n",
    "## 1. Introduction\n",
    "Overview: \n",
    "- Data: mnist \n",
    "- Methodology: multilayer perceptron (mlp)\n",
    "\n",
    "In this notebook we will use Keras to apply a deep multilayer perceptron on the famous MNIST data. In order to this we have to process the data in a way that the neural network can make use of them. Moreover, we have to specify a neural network representation that fits to the shape of our data and the objective of predicting outcome classes. \n",
    "\n",
    "The model gets to 98.14% test accuracy after 10 epochs with a margin for additional improvements (feature engineering, parameter tuning, etc.).\n",
    "\n",
    "## 2. The data\n",
    "The dataset we will use can be directly imported with the Keras API ([datasets](https://keras.io/datasets/)). We will use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) to train a neural network, which consists of images of handwritten digits from \"0\" to \"9\", normalized to fit into a 28x28 pixel box. This gives us a total of 784 input features ($x_1, x_2, ..., x_{784})$. In other words, 784 variables are associated with one observational outcome of a number between 0 and 9. Each input value $x_i$ is a number between 0-255, specifying the shade of gray on a [8-bit grayscale](https://en.wikipedia.org/wiki/Grayscale) at each place of the two dimenions of the image.\n",
    "\n",
    "Accordinlgy, the input layer of our neural network requires 784 nodes (one for each variable $x_1, x_2, ..., x_{784}$) and the output layer needs 10 nodes as each class is associated with one node.\n",
    "\n",
    "Moreover, the dataset consists of 60,000 training images, along with a seperate test set of 10,000 images. To download and seperate the data into training and testing data, use:\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "#### Preprocessing\n",
    "In order to model the data we need to process them in a certain way before passing them to the model. \n",
    "\n",
    "The dimensions of the imported dataset are a three-dimensional vector. For the training data we have a shape of (60000, 28, 28) and each observation has the shape (1, 28, 28) or 28x28 variables per image. In order to use them for our model we need to [reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) (aka flatten) our data. Each observation is covnerted from 28x28 to have only one dimension of 784 variables. The whole training data becomes (60000x784) or 60,000 observations with 784 variables.\n",
    "\n",
    "Additionally, it is common to use 32-bit precision when training a neural network and we will therefore convert the data to be 32 bit floats ([more](https://github.com/ageron/handson-ml/issues/265)). \n",
    "\n",
    "We also use [feature scaling](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) on our data. In particular, we normalize our data to narrow the range of values to a scale of 0 and 1. In general, gradient descent and other algoirithms converge faster with scaled features and may also improve the performance of the network (but not necessarily).  We perform normalization dividing by the maximum of 255.\n",
    "\n",
    "A last step is to convert the y into a categorical variable. To summarize preprocessing: \n",
    "\n",
    "- Flatten/reshape the data\n",
    "- Convert to type \"float 32\"\n",
    "- Normalize the data\n",
    "- Convert y to a categorical variable\n",
    "\n",
    "#### The model \n",
    "Next step is to implement a neural network and apply this model to the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Keras imports\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import utils\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data: \n",
      "x: (60000, 28, 28) y: (60000,)\n",
      "Shape test data: \n",
      "x: (10000, 28, 28) y: (10000,)\n",
      "Unique values in y:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Flattened shape x_train: (60000, 784)\n",
      "Flattened shape x_test: (10000, 784)\n",
      "Shape of y_train: (60000, 10)\n",
      "Shape of x_train: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Examine shape\n",
    "print(\"Shape training data:\", \"\\nx:\",x_train.shape, \"y:\",y_train.shape)\n",
    "print(\"Shape test data:\", \"\\nx:\",x_test.shape, \"y:\",y_test.shape)\n",
    "# print(\"Example of one observation (28x28 pixels):\\n\",x_train[123,:,:])\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts = True)\n",
    "print(\"Unique values in y: \", unique)\n",
    "\n",
    "# Flatten the data\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Change the type \n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "# Normalize the data\n",
    "max_x = int(x_train.max())\n",
    "x_train /= max_x\n",
    "x_test /= max_x\n",
    "\n",
    "# new shape\n",
    "print(\"Flattened shape x_train:\", x_train.shape)\n",
    "print(\"Flattened shape x_test:\", x_test.shape)\n",
    "\n",
    "# Convert y to categorical vector\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_train:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The form of the y_train seems a little unusual (60000, 10) as we have one dummy for each outcome, yielding ten outcome rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "After loading and preprocessing the data we can build a sequential model and feed it our data for optimization. In addition to the mentioned elements it includes [dropout regularization](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/). Dropout is a technique where a percentage of neurons are randomly shut off during training to reduce overfitting and in general improves the generalizability of the model. Previously we have used a list of layers to pass our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 0.2466 - acc: 0.9248 - val_loss: 0.1450 - val_acc: 0.9541\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.1018 - acc: 0.9692 - val_loss: 0.0934 - val_acc: 0.9710\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 23s 388us/step - loss: 0.0755 - acc: 0.9773 - val_loss: 0.0778 - val_acc: 0.9781\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 0.0614 - acc: 0.9818 - val_loss: 0.0719 - val_acc: 0.9801\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.0512 - acc: 0.9848 - val_loss: 0.0651 - val_acc: 0.9823\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0447 - acc: 0.9869 - val_loss: 0.0790 - val_acc: 0.9813\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 0.0409 - acc: 0.9882 - val_loss: 0.0802 - val_acc: 0.9810\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.0362 - acc: 0.9893 - val_loss: 0.0723 - val_acc: 0.9824\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0315 - acc: 0.9908 - val_loss: 0.0831 - val_acc: 0.9815\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 26s 434us/step - loss: 0.0300 - acc: 0.9912 - val_loss: 0.0921 - val_acc: 0.9814\n"
     ]
    }
   ],
   "source": [
    "# time\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "# Model parameters\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Neural network\n",
    "model = Sequential([\n",
    "    Dense(512, input_dim = 784),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(10),\n",
    "    Activation(\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile \n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = RMSprop(),\n",
    "              metrics = [\"accuracy\"] )\n",
    "\n",
    "# Fit the model \n",
    "model.fit(x_train, y_train, \n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs, \n",
    "          verbose = 1,\n",
    "          validation_data = (x_test, y_test),\n",
    "          shuffle = True)\n",
    "\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 204.75\n",
      "10000/10000 [==============================] - 2s 225us/step\n",
      "Test loss: 0.0921\n",
      "Test accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "# Time \n",
    "total_time = end_time - start_time\n",
    "print(\"Total running time:\", round(total_time,2))\n",
    "\n",
    "# Calculate score for test set\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0].round(4))\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
