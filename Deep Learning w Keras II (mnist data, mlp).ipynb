{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras II (mnist data, mlp)\n",
    "\n",
    "Overview: \n",
    "- Data: mnist \n",
    "- Methodology: multilayer perceptron (mlp)\n",
    "\n",
    "## 1. Introduction\n",
    "- Part I: [Deep Learning w Keras I (Intro)]()\n",
    "- In this notebook we will now use Keras to apply a deep multilayer perceptron on the MNIST data. These can be  imported using the Keras API ([datasets](https://keras.io/datasets/)). \n",
    "\n",
    "## 2. The data\n",
    "In the following notebook we will use the [MNIST data](https://en.wikipedia.org/wiki/MNIST_database) to train a neural network. The MNIST data consists of images of handwritten digits from \"0\" to \"9\", normalized to fit into a 28x28 pixel box. This gives us a total of $28\\cdot28 = 784$ input features ($x_1, x_2, ..., x_{784})$. In other words, 784 variables are associated with one observational outcome of a number from 0-9. Each input variable $x_i$ is a number between 0-255 , specifying the shade of gray on a [8-bit grayscale](https://en.wikipedia.org/wiki/Grayscale).\n",
    "\n",
    "For our neural network layers this means that the input layer requires 784 nodes (one for each variable $x_1, x_2, ..., x_{784}$) and the output layer has 10 nodes, each class is a number.\n",
    "\n",
    "Moreover, the dataset consists of 60,000 training images, along with a seperate test set of 10,000 images. To download and seperate the data into training and testing data, use:\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "#### Preprocessing\n",
    "In order to model the data we need to process them in a certain way before passing them to the model. \n",
    "\n",
    "The dimensions of the imported dataset are a three-dimensional vector. For the training data we have a shape of (60000, 28, 28) and each observation has the shape (1, 28, 28) or 28x28 variables per image. In order to use them for our model we need to [reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) (aka flatten) our data. Each observation is covnerted from 28x28 to have only one dimension of 784 variables. The whole training data becomes (60000x784) or 60,000 observations with 784 variables.\n",
    "\n",
    "Additionally, it is common to use 32-bit precision when training a neural network and we will therefore convert the data to be 32 bit floats ([more](https://github.com/ageron/handson-ml/issues/265)). \n",
    "\n",
    "We also use [feature scaling](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) on our data. In particular, we normalize our data to narrow the range of values to a scale of 0 and 1. In general, gradient descent and other algoirithms converge faster with scaled features and may also improve the performance of the network (but not necessarily).  We perform normalization dividing by the maximum of 255.\n",
    "\n",
    "A last step is to convert the y into a categorical variable. To summarize preprocessing: \n",
    "\n",
    "- Flatten/reshape the data\n",
    "- Convert to type \"float 32\"\n",
    "- Normalize the data\n",
    "- Convert y to a categorical variable\n",
    "\n",
    "#### The model \n",
    "Next step is to implement a neural network and apply this model to the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data: \n",
      "x: (60000, 28, 28) y: (60000,)\n",
      "Shape test data: \n",
      "x: (10000, 28, 28) y: (10000,)\n",
      "Unique values in y:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Flattened shape x_train: (60000, 784)\n",
      "Flattened shape x_test: (10000, 784)\n",
      "Shape of y_train: (60000, 10)\n",
      "Shape of x_train: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Examine shape\n",
    "print(\"Shape training data:\", \"\\nx:\",x_train.shape, \"y:\",y_train.shape)\n",
    "print(\"Shape test data:\", \"\\nx:\",x_test.shape, \"y:\",y_test.shape)\n",
    "# print(\"Example of one observation (28x28 pixels):\\n\",x_train[123,:,:])\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts = True)\n",
    "print(\"Unique values in y: \", unique)\n",
    "\n",
    "# Flatten the data\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Change the type \n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "# Normalize the data\n",
    "max_x = int(x_train.max())\n",
    "x_train /= max_x\n",
    "x_test /= max_x\n",
    "\n",
    "# new shape\n",
    "print(\"Flattened shape x_train:\", x_train.shape)\n",
    "print(\"Flattened shape x_test:\", x_test.shape)\n",
    "\n",
    "# Convert y to categorical vector\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_train:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The form of the y_train seems a little unusual (60000, 10) as we have one dummy for each outcome, yielding ten outcome rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "After loading and preprocessing the data we can build a sequential model and feed it our data for optimization. In addition to the mentioned elements it includes [dropout regularization](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/). Dropout is a technique where a percentage of neurons are randomly shut off during training to reduce overfitting and in general improves the generalizability of the model. Previously we have used a list of layers to pass our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.2480 - acc: 0.9243 - val_loss: 0.1310 - val_acc: 0.9603\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 0.1022 - acc: 0.9697 - val_loss: 0.0838 - val_acc: 0.9752\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 19s 310us/step - loss: 0.0759 - acc: 0.9767 - val_loss: 0.0909 - val_acc: 0.9752\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 0.0590 - acc: 0.9823 - val_loss: 0.0801 - val_acc: 0.9773\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0499 - acc: 0.9845 - val_loss: 0.0876 - val_acc: 0.9789\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0438 - acc: 0.9869 - val_loss: 0.0724 - val_acc: 0.9833\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0383 - acc: 0.9886 - val_loss: 0.0868 - val_acc: 0.9802\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 20s 338us/step - loss: 0.0344 - acc: 0.9902 - val_loss: 0.0811 - val_acc: 0.9816\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 20s 340us/step - loss: 0.0311 - acc: 0.9913 - val_loss: 0.0982 - val_acc: 0.9795\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.0294 - acc: 0.9913 - val_loss: 0.0847 - val_acc: 0.9826\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 19s 325us/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.0848 - val_acc: 0.9826\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 0.0244 - acc: 0.9928 - val_loss: 0.0966 - val_acc: 0.9836\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 0.0222 - acc: 0.9937 - val_loss: 0.0999 - val_acc: 0.9817\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 0.0904 - val_acc: 0.9830\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0917 - val_acc: 0.9827\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.1069 - val_acc: 0.9819\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0977 - val_acc: 0.9828\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0197 - acc: 0.9948 - val_loss: 0.1166 - val_acc: 0.9825\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.1050 - val_acc: 0.9836\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 21s 358us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.1057 - val_acc: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2971efc9c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Neural network\n",
    "model = Sequential([\n",
    "    Dense(512, input_dim = 784),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(10),\n",
    "    Activation(\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile \n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = RMSprop(),\n",
    "              metrics = [\"accuracy\"] )\n",
    "\n",
    "# Fit the model \n",
    "model.fit(x_train, y_train, \n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs, \n",
    "          verbose = 1,\n",
    "          validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 153us/step\n",
      "Test loss: 0.1057\n",
      "Test accuracy: 0.9836\n",
      "### The End ###\n"
     ]
    }
   ],
   "source": [
    "# Calculate score for test set\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0].round(4))\n",
    "print(\"Test accuracy:\", score[1])\n",
    "print(\"### The End ###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
