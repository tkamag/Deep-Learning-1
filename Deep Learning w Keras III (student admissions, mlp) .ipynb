{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras III (student admissions, mlp)\n",
    "\n",
    "Overview: \n",
    "- Data: Student admissions data\n",
    "- Methodology: Artifical neural network\n",
    " - Type: Multilayer perceptron\n",
    "\n",
    "## 1. Introduction\n",
    "- Part I:\n",
    "- Part II: \n",
    "In this notebook we apply a **mlp** neural network to student admissions data. The objective of the model is to build a model that predicts whether a student is admitted or not based on three explanatory variables.\n",
    "\n",
    "## 2. The data\n",
    "The dataset consists of 400 observations of students applying to university. Our **binary** outcome variable is whether the student was admitted or not (`admit`) and the three explanatory feature variables which will be used to predict the outcome are\n",
    "- `gre`: standardized test\n",
    "- `gpa`: academic achievement measure\n",
    "- `rank`: categorical rank of student\n",
    "\n",
    "### Preprocessing\n",
    "We have to undertake some steps in order to apply our multilayer perceptron model. Necessary preprocessing steps are\n",
    "- Normalization: Normalize the numeric feature variables (gre and gpa) so they are all in the same range between 0 and 1.\n",
    "- Convert to categorical: Rank is coded numerical in the data even though it is a categorical variable. Therefore, we will re-encode this variable by including dummies for each rank instead of the numerical value. This technique is known as [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/). \n",
    "\n",
    "In addition, we will split the data into training and testing data. Thus, we can evaluate the trained model fitted to the train data on previously unseen test data to avoid that the model's accuracy is simply a consequence of \"memorizing\" the test dataset.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import utils\n",
    "\n",
    "# set seed\n",
    "np.random.seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (400, 4)\n",
      "\n",
      "Head of initial data:\n",
      "    admit  gre   gpa  rank\n",
      "0      0  380  3.61     3\n",
      "1      1  660  3.67     3\n",
      "2      1  800  4.00     1\n",
      "3      1  640  3.19     4\n",
      "4      0  520  2.93     4\n",
      "\n",
      "Shape of processed data: (400, 7)\n",
      "\n",
      "Head of processed data:\n",
      "    admit    gre     gpa  rank_1  rank_2  rank_3  rank_4\n",
      "0      0  0.475  0.9025       0       0       1       0\n",
      "1      1  0.825  0.9175       0       0       1       0\n",
      "2      1  1.000  1.0000       1       0       0       0\n",
      "3      1  0.800  0.7975       0       0       0       1\n",
      "4      0  0.650  0.7325       0       0       0       1\n"
     ]
    }
   ],
   "source": [
    "# Import data from csv \n",
    "data = pd.read_csv(\"data/student_admissions.csv\")\n",
    "\n",
    "# Initial shape and head\n",
    "print(\"Shape of data:\",data.shape)\n",
    "print(\"\\nHead of initial data:\\n\",data.head())\n",
    "\n",
    "# Normalize to a range between 0 and 1\n",
    "data[\"gre\"] = data[\"gre\"] / data[\"gre\"].max()\n",
    "data[\"gpa\"] = data[\"gpa\"] / data[\"gpa\"].max()\n",
    "\n",
    "# get dummies for rank\n",
    "dummies = pd.get_dummies(data[\"rank\"], prefix=\"rank\")\n",
    "\n",
    "# Dropping numerical \"rank\", concatenating data and dummies\n",
    "data = pd.concat([data.drop(\"rank\", axis=1), dummies], axis=1, sort=False)\n",
    "\n",
    "# print new shape and head\n",
    "print(\"\\nShape of processed data:\", data.shape)\n",
    "print(\"\\nHead of processed data:\\n\",data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step:**\n",
    "\n",
    "Using a function to split the data into train and test set and converting them to a numpy array. In addition, we still have to re-encode the y-variable using a dummy for each category (two in this case).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      " [[0.925  1.     0.     0.     1.     0.    ]\n",
      " [0.825  0.7275 0.     0.     1.     0.    ]\n",
      " [0.725  0.865  0.     0.     0.     1.    ]\n",
      " [0.85   0.8275 0.     1.     0.     0.    ]\n",
      " [0.775  0.9025 1.     0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "# Train_test function\n",
    "def train_test(df, test_size):\n",
    "    idx = df.index\n",
    "    test_len = int(len(idx)*test_size)\n",
    "    sample = np.random.choice(idx, size = test_len, replace = False)\n",
    "    train = df.iloc[sample]\n",
    "    test = df.drop(sample)\n",
    "    return train, test\n",
    "\n",
    "# train, test - split\n",
    "train_data, test_data = train_test(data, 0.8)\n",
    "\n",
    "# Split into y_train, x_train\n",
    "y_train = np.array(utils.to_categorical(train_data[\"admit\"], num_classes=2))\n",
    "x_train = np.array(train_data.drop(columns=[\"admit\"]))\n",
    "\n",
    "# Split into y_test, x_test, \n",
    "y_test = np.array(utils.to_categorical(test_data[\"admit\"], num_classes=2))\n",
    "x_test = np.array(test_data.drop(columns=[\"admit\"]))\n",
    "\n",
    "# print results \n",
    "print(y_train[:5])\n",
    "print(\"\\n\",x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The model\n",
    "We are using a multilayer perceptron neural network for this binary problem. Before we have built the model by specifying it in a list to the `Sequential()`-function. Now, we are adding layers sequentially using the `.add()`-function. This is another option to specify the network.\n",
    "\n",
    "Other compiler options: \n",
    "- [Loss functions](https://keras.io/losses/): mean_squared_error, binary_crossentropy, categorical_crossentropy, ...\n",
    "- [Optimizers](https://keras.io/optimizers/): adam, rmsprop, sgd, ...\n",
    "- [Metrics](https://keras.io/metrics/): accuracy, mae, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 3.6\n",
      "320/320 [==============================] - 0s 25us/step\n",
      "80/80 [==============================] - 0s 87us/step\n",
      "\n",
      "Training Accuracy: 0.7219\n",
      "Testing Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "# time \n",
    "from time import time\n",
    "start_model = time()\n",
    "\n",
    "# Initializing model object\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers\n",
    "model.add(Dense(128, activation = \"relu\", input_dim = x_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "# Compiler\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x = x_train, y = y_train,\n",
    "          epochs = 256,\n",
    "          batch_size = 128, \n",
    "          verbose = 0,\n",
    "          validation_data = (x_test, y_test))\n",
    "\n",
    "end_model = time()\n",
    "time_model = end_model - start_model\n",
    "print(\"Total running time:\", round(time_model, 2))\n",
    "\n",
    "# Evaluate model\n",
    "train_score = model.evaluate(x_train, y_train)\n",
    "test_score = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTraining Accuracy:\", train_score[1].round(4))\n",
    "print(\"Testing Accuracy:\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "A testing accuracy below 70% seems not to be overwhelming. However, this is a fairly small dataset that is used to train the model (320 training observations) and there is a lot of margin for parameter training left. It may also be a good idea to compare the performance to supervised machine learning algorithms like random forest, which may outperform the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next steps:**\n",
    "- Compare to traditional supervised ML algorithms\n",
    "- Visualize training, testing accuracy over time.\n",
    "- Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
