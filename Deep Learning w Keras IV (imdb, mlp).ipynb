{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras IV (imdb, mlp)\n",
    "- Part I: [Deep Learning w Keras I (Intro)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20I%20(intro).ipynb)\n",
    "- Part II: [Deep Learning w Keras II (mnist data, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20II%20(mnist%20data%2C%20mlp).ipynb)\n",
    "- Part III: [Deep Learning w Keras III (student admissions, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20Keras%20III%20(student%20admissions%2C%20mlp)%20.ipynb)\n",
    " \n",
    "## 1. Introduction\n",
    "Overview:  \n",
    "- Data: IMDB movie reviews sentiment classification\n",
    "- Methodology: Artificial neural network\n",
    " - Type: Multilayer perceptron\n",
    "\n",
    "This notebook shows how to implement a simple neural network (multilayer perceptron) to imdb movie review data. The outcome variable is whether a review is perceived as positive or negative. Our objective is to fit a model which identifies positive/negative reviews based on the text content of the review.\n",
    "\n",
    "The model gets to 85.55% test accuracy after 10 epochs with margin for additional improvements (feature engineering, parameter tuning, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import utils\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The data\n",
    "We will import the [dataset](https://keras.io/datasets/) using the keras API. The dataset consists of 25,000 movies reviews from IMDB which are already vectorized. The outcome variable we are trying to model is whether a review is positive or negative. Words are indexed by overall frequency in the dataset so that the vectorized word of \"3\" means it is the 3rd most frequent word in the entire dataset. Unknown words are encoded as \"0\".\n",
    "\n",
    "### Preprocessing \n",
    "1. Convert words in x to a matrix of dummy variables.\n",
    " - [Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) using Keras' [tokenizer()](https://keras.io/preprocessing/text/) class.\n",
    " - convert to matrix using [sequences_to_matrix](https://keras.rstudio.com/reference/sequences_to_matrix.html).\n",
    "2. Convert y to categorical using [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical).\n",
    "\n",
    "Note that the data has already been preprocessed in a way that each word is encoded as how frequent the word occured in the dataset. The most frequent word is encoded as \"1\", the second most frequent word as \"2\" and so forth. \n",
    "\n",
    "We still have to do some preprocessing to prepare the data for our model. First, we need to convert the encoded text into a format that can be used by our algorithm. This is also known one-hot encoding, where each word becomes a dummy variable yielding a huge feature matrix with 1000 variables. The second step is to convert y into a binary matrix representation which is then used as outcome matrix to fit the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape x_train: (25000,)\n",
      "Shape x_test: (25000,)\n",
      "Shape y_train: (25000,)\n",
      "Shape y_test: (25000,)\n",
      "Length of example review: 272\n",
      "First five words in example:  [1, 14, 9, 24, 6]\n",
      "First five observations for y_train:  [1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load data with only the 1000 most frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000,\n",
    "                                                      seed=42)\n",
    "\n",
    "print(\"Shape x_train:\", x_train.shape)\n",
    "print(\"Shape x_test:\", x_test.shape)\n",
    "print(\"Shape y_train:\", y_train.shape)\n",
    "print(\"Shape y_test:\", y_test.shape)\n",
    "\n",
    "# Show characteristics of example review\n",
    "print(\"Length of example review:\", len(x_train[16]))\n",
    "print(\"First five words in example: \",x_train[16][:5])\n",
    "\n",
    "# y_train \n",
    "print(\"First five observations for y_train: \",y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "\n",
    "# One-hot encode x\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode=\"binary\")\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode=\"binary\")\n",
    "\n",
    "# One-hot encode y\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes = 2)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (25000, 1000)\n",
      "Head of first text in x_train: [0. 1. 1. 0. 1.]\n",
      "Shape of y_train: (25000, 2)\n",
      "Head of y_train:\n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check x,y after processing\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Head of first text in x_train:\", x_train[0][:5])\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Head of y_train:\\n\", y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Model\n",
    "Now, we can build a neural network and fit the data to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, activation = \"relu\", input_shape = (1000,)))\n",
    "model.add(Dropout(0.5))\n",
    "# Hidden layer\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# Output layer\n",
    "model.add(Dense(2, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer = \"rmsprop\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, batch_size = 64,\n",
    "          epochs = 10,\n",
    "          verbose = 0,\n",
    "          validation_data = (x_test, y_test),\n",
    "          shuffle = False)\n",
    "\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 27.1824\n",
      "Train loss: 0.1495\n",
      "Train accuracy: 0.9575\n",
      "Test loss: 0.4647\n",
      "Test accuracy: 0.8539\n"
     ]
    }
   ],
   "source": [
    "# Runnng time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total running time:\", round(total_time,4))\n",
    "\n",
    "# Training set\n",
    "train_score = model.evaluate(x_train, y_train, verbose = 0)\n",
    "print(\"Train loss:\",train_score[0].round(4))\n",
    "print(\"Train accuracy:\", train_score[1].round(4))\n",
    "\n",
    "# Test set \n",
    "test_score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print(\"Test loss:\",test_score[0].round(4))\n",
    "print(\"Test accuracy:\", test_score[1].round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
