{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch I (intro)\n",
    "- Part II: [Deep Learning w PyTorch II (define a nn)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20II%20(define%20a%20nn).ipynb)\n",
    "- Part III: [Deep Learning w PyTorch III (training nn, theory)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20III%20(training%20nn%2C%20theory).ipynb)\n",
    "- Part IV: [Deep Learning w PyTorch IV (mnist, mlp)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20IV%20(mnist%2C%20mlp).ipynb)\n",
    "- Part V: [Deep Learning w PyTorch V (fmnist, mlp, inference and validation)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20V%20(fmnist%2C%20mlp%2C%20inference%20and%20validation).ipynb)\n",
    "\n",
    "## 1. Introduction\n",
    "This series of notebooks briefly introduces the basics of deep learning (DL) using PyTorch. After introducing the respective building blocks that are necessary to define and train a network, we will apply these for prediction.\n",
    "\n",
    "DL is based on so-called neural networks. A neural network consists of individual units (which ought to resemble a neuron in the human brain). Each of these units has a number of weighted inputs which are summed together as a linear combination. Those weighted sums are then passed to an activation function to get the unit's output. \n",
    "\n",
    "<img src=\"images/one_neuron.png\">\n",
    "\n",
    "This can be formulated in the simple case of two layers as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or in vectorized form ([dot product](https://en.wikipedia.org/wiki/Dot_product), i.e. inner product of two vectors): \n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Ressources: Mathematics\n",
    "There are two branches of mathematics that neural networks heavily relies on. \n",
    "1. Linear Algebra\n",
    "2. Multivariable Calculus \n",
    "\n",
    "Ressources to revisit some concepts: \n",
    "- [Essence of linear algebra (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "- [Essence of calculus (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n",
    "- [Neural Networks (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "For a more academic approach towards the mathematical aspects:\n",
    "- [MIT OCW: Linear Algebra (18.06SC)](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/)\n",
    "- [MIT OCW: Single Variable Calculus (18.01SC)](https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/)\n",
    "- [MIT OCW: Multivariable Calculus (18.02SC)](https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm)\n",
    "\n",
    "#### Introducing tensors\n",
    "Neural networks are based on linear algebra operations, especially on tensors. Tensors are a generalization of matrices in the sense that they allow to have more than two dimensions:\n",
    "- Vector: 1-dimensional tensor\n",
    "- Matrix: 2-dimensional tensor\n",
    "- Higher order arrays: n-dimensional tensors. \n",
    "\n",
    "<img src=\"images/Tensor.PNG\">\n",
    "\n",
    "Intuititively, a 3-dimensional tensor is like a cube where behind every matrix-like structure other numbers are hidden. A rubic's cube can be called a tensor of dimensions [3,3,3]. Working with tensors in python requires `torch` which provides a way to work with an n-dimensional array (tensor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "\n",
    "# check version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors in Python\n",
    "Tensors in Python can be thought of as matrices in higher dimensions. A 3x2x2 tensor can be thought of as 3 stacked 2x2 matrices and so forth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (3x4):\n",
      " tensor([[-0.0065,  2.8250,  1.0579,  0.0723],\n",
      "        [-1.1421, -0.5649, -0.2128,  1.3161],\n",
      "        [ 1.3855, -0.2905,  0.0922, -0.5330]])\n",
      "\n",
      " tensor(-0.2905)\n",
      "\n",
      "b (3x3x3):\n",
      " tensor([[[ 1.1005,  1.4149,  1.0838],\n",
      "         [-1.1407,  0.2112,  0.3487],\n",
      "         [ 0.8094,  0.0327,  0.2239]],\n",
      "\n",
      "        [[ 0.0356, -1.0853,  0.1023],\n",
      "         [ 0.5790, -1.6186,  0.1011],\n",
      "         [ 1.7942, -0.0531, -1.7610]],\n",
      "\n",
      "        [[ 2.0220, -0.5716, -0.7366],\n",
      "         [ 1.6217,  0.0141,  1.6517],\n",
      "         [-0.9199, -0.2983, -0.7970]]])\n",
      "\n",
      " tensor(-0.5716)\n",
      "\n",
      "c (5x3):\n",
      " tensor([[0.7249, 0.9768, 0.8921],\n",
      "        [0.3772, 0.2931, 0.1563],\n",
      "        [0.0411, 0.2572, 0.8437],\n",
      "        [0.7373, 0.7870, 0.3610],\n",
      "        [0.7794, 0.0548, 0.0207]])\n"
     ]
    }
   ],
   "source": [
    "# 3x4 array.\n",
    "a = torch.randn(3,4)\n",
    "print(\"a (3x4):\\n\",a)\n",
    "print(\"\\n\",a[2,1])\n",
    "\n",
    "# 3x3x3 array\n",
    "b = torch.randn(3,3,3)\n",
    "print(\"\\nb (3x3x3):\\n\",b)\n",
    "print(\"\\n\",b[2,0,1])\n",
    "\n",
    "# 5x3 array\n",
    "c = torch.rand(5,3)\n",
    "print(\"\\nc (5x3):\\n\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple network with linear algbera\n",
    "Using linear algebra to show a simple network with sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229]]) \n",
      "w:\n",
      " tensor([[-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]]) \n",
      "bias:\n",
      " tensor([[0.5349]])\n",
      "\n",
      "y:\n",
      " tensor([[0.6018]])\n",
      "\n",
      "y_mm:\n",
      " tensor([[0.6018]])\n"
     ]
    }
   ],
   "source": [
    "# sigmoid activation\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\n",
    "    Arguments\n",
    "    ---------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# random data x \n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1,5)\n",
    "\n",
    "# weights\n",
    "w = torch.randn_like(x)\n",
    "\n",
    "# bias (intercept)\n",
    "bias = torch.randn(1,1)\n",
    "\n",
    "print(\"x:\\n\",x,\"\\nw:\\n\",w,\"\\nbias:\\n\",bias)\n",
    "\n",
    "# Calculate the output\n",
    "h = torch.sum(x*w) + bias\n",
    "y = sigmoid(h)\n",
    "print(\"\\ny:\\n\",y)\n",
    "\n",
    "# Using matrix multiplication\n",
    "y_mm = sigmoid(torch.mm(w, x.reshape(5,1)) + bias)\n",
    "print(\"\\ny_mm:\\n\", y_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change size of matrix\n",
    "For some operations it will become important to change the size of the matrix, i.e. for flattening or using the transpose of a matrix: \n",
    "\n",
    "* `w.reshape(a, b)` will return a new tensor with the same data as `w` with size `(a, b)` sometimes, and sometimes a clone, as it copies the data to another part of memory.\n",
    "* `w.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here it should be noted that the underscore at the end of the method denotes that this method is performed [**in-place**](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `w.view(a, b)` will return a new tensor with the same data as `w` with size `(a, b)`.\n",
    "* In general, it is a good advice to use these methods only to flip a matrix over its diagonal leading to a [transpose](https://en.wikipedia.org/wiki/Transpose) of the matrix or for flattening (i.e. reducing dimensionality from 2D array to 1D array).\n",
    "\n",
    "#### Stacking matrices\n",
    "The real fun starts when stacking individual units into layers and stacks of layers, into a network of neurons. Matrix algebra is necessary here as we are now working with matrices instead of vectors.\n",
    "\n",
    "<img src=images/stacking_weights.png>\n",
    "\n",
    "The hidden layer ($h_1$ and $h_2$ here) can be calculated as\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network can be found by treating the hidden layer as inputs for the output unit. The network output is expressed as\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network with hidden layer in matrix notation\n",
    "- 3 variables $x_1, x_2, x_3$ per observation: Input vector of size (1x3) leading to a matrix of (kx3).\n",
    "- 1 output: Hence, 1 output node. The output value is then passed to an activation function. \n",
    "- Weights are calculated from input to output using matrix calculations (Feedforward). We are starting with random weights. \n",
    "- In every layer there is a bias which can be thought of as error like in a regression model.\n",
    "\n",
    "Steps: \n",
    "- Initiate weight matrices of the right dimensions (with random numbers for now).\n",
    "- Calculate output\n",
    "\n",
    "Matrix dimensions:\n",
    "- W1 (3,2) -> Input variables to hidden layer\n",
    "- W2 (2,1) -> hidden layer to output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " tensor([[ 0.2303, -1.1229],\n",
      "        [-0.1863,  2.2082],\n",
      "        [-0.6380,  0.4617]])\n",
      "W2:\n",
      " tensor([[0.2674],\n",
      "        [0.5349]])\n",
      "B1:\n",
      " tensor([[0.8094, 1.1103]])\n",
      "B2:\n",
      " tensor([[-1.6898]])\n",
      "\n",
      "hidden layer h:\n",
      " tensor([[0.6711, 0.7549]])\n",
      "\n",
      "output:\n",
      " tensor([[0.2485]])\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# one observation for x\n",
    "x = torch.randn(1,3)\n",
    "\n",
    "# Size of each layer\n",
    "n_input = x.shape[1]\n",
    "n_hidden = 2 \n",
    "n_output = 1\n",
    "\n",
    "# Weight matrices \n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# Biases for both layers\n",
    "B1 = torch.randn(1, n_hidden)\n",
    "B2 = torch.randn(1, n_output)\n",
    "\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"B1:\\n\", B1)\n",
    "print(\"B2:\\n\", B2)\n",
    "\n",
    "# Calculate output\n",
    "h = sigmoid(torch.mm(x, W1) + B1)\n",
    "print(\"\\nhidden layer h:\\n\", h)\n",
    "output = sigmoid(torch.mm(h,W2) + B2)\n",
    "print(\"\\noutput:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion NumPy to Torch and back\n",
    "Converting between numpy arrays and torch tensor use `torch.from_numpy()`. To convert a tensor to a numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      " [[0.58265814 0.13556693 0.9434121 ]\n",
      " [0.95882513 0.11286997 0.89693295]]\n",
      "e:\n",
      " tensor([[0.5827, 0.1356, 0.9434],\n",
      "        [0.9588, 0.1129, 0.8969]], dtype=torch.float64)\n",
      "\n",
      "torch to numpy of e:\n",
      " [[0.58265814 0.13556693 0.9434121 ]\n",
      " [0.95882513 0.11286997 0.89693295]]\n",
      "\n",
      "e*2:\n",
      " tensor([[1.1653, 0.2711, 1.8868],\n",
      "        [1.9177, 0.2257, 1.7939]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# random numbers\n",
    "d = np.random.rand(2,3)\n",
    "e = torch.from_numpy(d)\n",
    "\n",
    "print(\"d:\\n\", d)\n",
    "print(\"e:\\n\", e)\n",
    "print(\"\\ntorch to numpy of e:\\n\", e.numpy())\n",
    "print(\"\\ne*2:\\n\",e.mul_(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is shared between np array and torch tensor (aliasing), so if you change the values in-place of one object, the other will change as well.\n",
    "\n",
    "Regarding underscores: \n",
    "[in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244)\n",
    "`\"_\"`denotes that the method is performed in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "In the next notbeok we will see how a neural network can be defined with PyTorch. In particular, how a multilayer perceptron network can be formulated that can be used to identify simple images like from the MNIST or Fashion-MNIST data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
