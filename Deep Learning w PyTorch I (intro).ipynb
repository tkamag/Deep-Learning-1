{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch I (intro)\n",
    "In this notebook we will briefly lay out the basics of deep learning and apply this to PyTorch. In upcoming parts these basics will be applied to various datasets. \n",
    "\n",
    "## 1. Introduction\n",
    "DL is based on so-called neural networks. A neural netword consists of individual units (which ought to resemble a neuron in the human brain but that's a different story). Each of these units has a number of weighted inputs which are summed together as a linear combination. Those weighted sums are then passed to an activation function to get the unit's output. \n",
    "\n",
    "<img src=\"images/one_neuron.png\">\n",
    "\n",
    "This can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or in vectorized form ([dot product](https://en.wikipedia.org/wiki/Dot_product)/ inner product of two vectors): \n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Ressources: Mathematics\n",
    "There are two branches of mathematics that neural networks heavily relies on. \n",
    "1. Linear Algebra\n",
    "2. Multivariable Calculus \n",
    "\n",
    "Ressources to revisit some concepts: \n",
    "- [Essence of linear algebra (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "- [Essence of calculus (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n",
    "- [Neural Networks (3B1B)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "For a more academic approach towards the mathematical aspects:\n",
    "- [MIT OCW: Linear Algebra (18.06SC)](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/)\n",
    "- [MIT OCW: Single Variable Calculus (18.01SC)](https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/)\n",
    "- [MIT OCW: Multivariable Calculus (18.02SC)](https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm)\n",
    "\n",
    "#### Introducing tensors\n",
    "Neural networks are based on linear algebra operations, especially on tensors. Tensors are a generalization of matrices in the sense that they allow to have more than two dimensions:\n",
    "- Vector: 1-dimensional tensor\n",
    "- Matrix: 2-dimensional tensor\n",
    "- Higher order arrays are then n-dimensional tensors. \n",
    "\n",
    "<img src=\"images/Tensor.PNG\">\n",
    "\n",
    "Intuititively, a 3-dimensional tensor is like a cube where behind every matrix-like structure other numbers are hidden. A rubic's cube can be called a tensor of dimensions [3,3,3]. Working with tensors in python requires `torch` which provides a way to work with an n-dimensional array or tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "\n",
    "# check version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[-1.6941,  0.5550,  1.3615,  1.3938],\n",
      "        [-0.0196, -0.7675, -1.1396,  0.2975],\n",
      "        [ 0.6880, -1.3425,  0.3506, -0.4811]])\n",
      "\n",
      " tensor(-1.3425)\n",
      "\n",
      "b:\n",
      " tensor([[[-0.4647,  0.2080,  0.0416],\n",
      "         [-0.7873, -0.4077,  1.2676],\n",
      "         [-0.5468, -0.0464,  1.6838]],\n",
      "\n",
      "        [[ 0.4631, -0.8512, -0.2336],\n",
      "         [ 0.8787,  0.1920, -1.5391],\n",
      "         [-0.8567,  0.8818,  1.6534]],\n",
      "\n",
      "        [[ 1.4839, -0.8139, -1.2964],\n",
      "         [ 1.1480, -0.2960, -0.0967],\n",
      "         [-0.3169, -0.9285, -0.8230]]])\n",
      "\n",
      " tensor(-0.8139)\n",
      "\n",
      "c:\n",
      " tensor([[0.0887, 0.0697, 0.3258],\n",
      "        [0.9512, 0.6808, 0.4619],\n",
      "        [0.0280, 0.1513, 0.2181],\n",
      "        [0.3980, 0.1852, 0.5710],\n",
      "        [0.3654, 0.3728, 0.4626]])\n"
     ]
    }
   ],
   "source": [
    "# 3x4 array.\n",
    "a = torch.randn(3,4)\n",
    "print(\"a:\\n\",a)\n",
    "print(\"\\n\",a[2,1])\n",
    "\n",
    "# 3x3x3 array\n",
    "b = torch.randn(3,3,3)\n",
    "print(\"\\nb:\\n\",b)\n",
    "print(\"\\n\",b[2,0,1])\n",
    "\n",
    "# 5x3 array\n",
    "c = torch.rand(5,3)\n",
    "print(\"\\nc:\\n\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple network with linear algbera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229]]) \n",
      "w:\n",
      " tensor([[-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]]) \n",
      "bias:\n",
      " tensor([[0.5349]])\n",
      "\n",
      "y:\n",
      " tensor([[0.6018]])\n",
      "\n",
      "y_mm:\n",
      " tensor([[0.6018]])\n"
     ]
    }
   ],
   "source": [
    "# sigmoid activation\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\n",
    "    Arguments\n",
    "    ---------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# random data x \n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1,5)\n",
    "\n",
    "# weights\n",
    "w = torch.randn_like(x)\n",
    "\n",
    "# bias (intercept)\n",
    "bias = torch.randn(1,1)\n",
    "\n",
    "print(\"x:\\n\",x,\"\\nw:\\n\",w,\"\\nbias:\\n\",bias)\n",
    "\n",
    "# Calculate the output\n",
    "h = torch.sum(x*w) + bias\n",
    "y = sigmoid(h)\n",
    "print(\"\\ny:\\n\",y)\n",
    "\n",
    "# Using matrix multiplication\n",
    "y_mm = sigmoid(torch.mm(w, x.reshape(5,1)) + bias)\n",
    "print(\"\\ny_mm:\\n\", y_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change size of matrix\n",
    "* `w.reshape(a, b)` will return a new tensor with the same data as `w` with size `(a, b)` sometimes, and sometimes a clone, as it copies the data to another part of memory.\n",
    "* `w.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here it should be noted that the underscore at the end of the method denotes that this method is performed [**in-place**](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `w.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "* In general, it is a good advice to use these methods only to flip a matrix over its diagonal leading to a [transpose](https://en.wikipedia.org/wiki/Transpose) of the matrix.\n",
    "\n",
    "#### Stacking matrices\n",
    "The real fun starts when stacking individual units into layers and stacks of layers, into a network of neurons. Matrix algebra becomes particularly useful here as it looks almost like the previously introduced notation with the difference that we are now working with matrices instead of vectors.\n",
    "\n",
    "<img src=images/stacking_weights.png>\n",
    "\n",
    "The hidden layer ($h_1$ and $h_2$ here) can be calculated as\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network with hidden layer in matrix notation\n",
    "- 3 variables $x_1, x_2, x_3$ per observation: Input vector of size (1x3) leading to a matrix of (kx3).\n",
    "- 1 output: Hence, 1 output node. The output value is then passed to an activation function. \n",
    "- Weights are calculated from input to output using matrix calculations (Feedforward). We are starting with random weights. \n",
    "- In every layer there is a bias which can be thought of as error like in a regression model.\n",
    "\n",
    "Steps: \n",
    "- Initiate weight matrices of the right dimensions (with random numbers for now).\n",
    "- Calculate output\n",
    "\n",
    "Matrix dimensions:\n",
    "- W1 (3,2) -> Input variables to hidden layer\n",
    "- W2 (2,1) -> hidden layer to output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " tensor([[ 0.2303, -1.1229],\n",
      "        [-0.1863,  2.2082],\n",
      "        [-0.6380,  0.4617]])\n",
      "W2:\n",
      " tensor([[0.2674],\n",
      "        [0.5349]])\n",
      "B1:\n",
      " tensor([[0.8094, 1.1103]])\n",
      "B2:\n",
      " tensor([[-1.6898]])\n",
      "\n",
      "hidden layer h:\n",
      " tensor([[0.6711, 0.7549]])\n",
      "\n",
      "output:\n",
      " tensor([[0.2485]])\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# one observation for x\n",
    "x = torch.randn(1,3)\n",
    "\n",
    "# Size of each layer\n",
    "n_input = x.shape[1]\n",
    "n_hidden = 2 \n",
    "n_output = 1\n",
    "\n",
    "# Weight matrices \n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# Biases for both layers\n",
    "B1 = torch.randn(1, n_hidden)\n",
    "B2 = torch.randn(1, n_output)\n",
    "\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"B1:\\n\", B1)\n",
    "print(\"B2:\\n\", B2)\n",
    "\n",
    "# Calculate output\n",
    "h = sigmoid(torch.mm(x, W1) + B1)\n",
    "print(\"\\nhidden layer h:\\n\", h)\n",
    "output = sigmoid(torch.mm(h,W2) + B2)\n",
    "print(\"\\noutput:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion NumPy to Torch and back\n",
    "Converting between numpy arrays and torch tensor use `torch.from_numpy()`. To convert a tensor to a numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      " [[0.9447224  0.34212498 0.19210632]\n",
      " [0.62806719 0.292273   0.31409224]]\n",
      "e:\n",
      " tensor([[0.9447, 0.3421, 0.1921],\n",
      "        [0.6281, 0.2923, 0.3141]], dtype=torch.float64)\n",
      "\n",
      "torch to numpy of e:\n",
      " [[0.9447224  0.34212498 0.19210632]\n",
      " [0.62806719 0.292273   0.31409224]]\n",
      "\n",
      "e*2:\n",
      " tensor([[1.8894, 0.6842, 0.3842],\n",
      "        [1.2561, 0.5845, 0.6282]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# random numbers\n",
    "d = np.random.rand(2,3)\n",
    "e = torch.from_numpy(d)\n",
    "\n",
    "print(\"d:\\n\", d)\n",
    "print(\"e:\\n\", e)\n",
    "print(\"\\ntorch to numpy of e:\\n\", e.numpy())\n",
    "print(\"\\ne*2:\\n\",e.mul_(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is shared between np array and torch tensor (aliasing), so if you change the values in-place of one object, the other will change as well.\n",
    "\n",
    "Regarding underscores: \n",
    "[in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244)\n",
    "Denotes that the method is performed in-place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
