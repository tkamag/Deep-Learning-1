{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning w PyTorch III (training neural networks)\n",
    "Previous parts showed how to build and represent a neural network, i.e. a multilayer perceptron.This part covers the actual training of that model. The training or fitting of the model to the data consists of adjusting the weights in the model in a way that the structure of the underlying data-generating process is covered and the model is able to predict the outcome. \n",
    "\n",
    "- Part 1\n",
    "- Part 2\n",
    "\n",
    "## 1. Introduction and Theory\n",
    "The previous network was untrained and couldn't therefore predict anything. Neural networks with non-linear activations work like function approximators by mapping the input to the output.\n",
    "\n",
    "<img src=\"images/mapping.png\">\n",
    "\n",
    "To train the network we usually start with random weights and assessing its performance. In each step we assess the performance of the model with the current weights and then adjust the the weights.\n",
    "\n",
    "#### Loss function\n",
    "This is done by calculating a loss using a **loss function** (also called cost function, [link](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)), which is a measure of our prediction error. For example, the mean squared (MS) loss is often used in regression and binary classification problems. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\large Loss_{MS} =  \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Gradient Descent: Minimize Loss\n",
    "Our goal is then to minimize this loss iteratively wrt the network parameters (the weights). We find this minimum using a process called [gradient descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent). \n",
    "\n",
    "The gradient is the slope of the loss function and points in the direction of steepest ascent of this function. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). A widely used analogy is to imagine that we are standing on a mountain and the gradient points towards the direction of steepest descent. We take therefore steps towards this promising direction to find a way down to the reach the valley.\n",
    "\n",
    "#### Backpropagation: Adjust weights of the network\n",
    "Training multilayer networks is done through backpropagation which is an iterative application of the chain rule. \n",
    "<img src=\"images/backward_pass.png\">\n",
    "3Blue1Brown provides a series of nice visualizations on neural networks, also covering [backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&t=0s). In backpropagation we first pass forward the input through layers to get the error/cost of the function (left side of the image). Afterwards, we adjust the weights and biases to minimize the loss. To adjsust the weights with gradient descent, we propagate the gradient of the loss backwards through the network by iteratively applying the chain rule: \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$ as we are taking only a small step towards the steepest descent of the function. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "## 2. Loss in PyTorch\n",
    "- PyTorch provides losses such as the cross-entropy loss (`nn.Cross.EntropyLoss`). The loss is usually assigned to `criterion`. \n",
    "- To actually calculate the loss, you first define the criterion then pass in the output of your network and the correct labels. \n",
    "- Note that we need to pass the raw output of our network into the loss, not the output of the softmax function (The raw output is usually called score or logits).\n",
    "\n",
    "Something really important to note here. Looking at [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "- This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "- The input is expected to contain scores for each class.\n",
    "\n",
    "We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network in PyTorch and Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# The network, no output activation function\n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define Loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load one Batch of images\n",
    "Next, \n",
    "- we'll load one batch of 64 images and respective labels, \n",
    "- flatten the images and \n",
    "- pass them through the uninitialized model leading to a score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images:\n",
      " torch.Size([64, 1, 28, 28]) \n",
      "Shape of labels:\n",
      " torch.Size([64])\n",
      "Reshaped images:\n",
      " torch.Size([64, 784])\n",
      "Shape of scores:\n",
      " torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# One batch of 64 images\n",
    "images, labels = next(iter(trainloader))\n",
    "print(\"Shape of images:\\n\", images.shape, \\\n",
    "      \"\\nShape of labels:\\n\",labels.shape)\n",
    "\n",
    "# Flattening the images\n",
    "images = images.reshape(images.shape[0],-1)\n",
    "print(\"Reshaped images:\\n\", images.shape)\n",
    "\n",
    "# Forward Pass: Images are passed through untrained network\n",
    "scores = model(images)\n",
    "print(\"Shape of scores:\\n\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use a loss function to map the inputs to a value which indicates the loss this batch given the current weights. The inputs are:\n",
    "- 10 scores per image (i.e. one score per class)\n",
    "- 1 label of the actual class. \n",
    "\n",
    "We'll be using [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss) and the inputs are 64x10 image scores and 64 labels. Cross-Entropy Loss are averaged across obersvations. This loss function formula can be described as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "loss \\,(x, \\,class) &= - log \\bigg( \\frac{exp(x[class])}{\\sum_j exp(x[j])} \\bigg) \\\\\n",
    "&= -x[class] + log \\bigg( \\sum_j exp(x[j] \\bigg)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:\n",
      " tensor(2.3298, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "loss = criterion(scores, labels)\n",
    "print(\"Average loss:\\n\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often covnentient to build a model with log-softmax output using using `nn.LogSoftmax` or `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Then we can get the actual probabilities by taking the exponential `torch.exp(output)`. With a log-softmax output, we want to use the negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)).\n",
    "\n",
    "Next step is to build a model returns the log-softmax as the output so we can calculate the loss using the negative log likelihood loss. \n",
    "\n",
    "Note that for `nn.LogSoftmax` and `F.log_softmax` we'll need to set the `dim` keyword argument appropriately. `dim=0` calculates softmax across the rows, so each column sums to 1, while `dim=1` calculates across the columns so each row sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for each number.:\n",
      "\n",
      "0: 9.9516%\n",
      "1: 9.2876%\n",
      "2: 10.9505%\n",
      "3: 8.5246%\n",
      "4: 11.2233%\n",
      "5: 10.5715%\n",
      "6: 8.5179%\n",
      "7: 10.7461%\n",
      "8: 10.5533%\n",
      "9: 9.6734%\n"
     ]
    }
   ],
   "source": [
    "# same network with output activation\n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Data \n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Forward Pass\n",
    "output_scores = model(images)\n",
    "\n",
    "# Calculate los\n",
    "loss = criterion(output_scores, labels)\n",
    "\n",
    "# Convert output scores to p\n",
    "probs = torch.exp(output_scores[0]).detach().numpy()\n",
    "print(\"Probabilities for each number.:\\n\")\n",
    "for i in range(len(probs)):\n",
    "    print( str(i) + \": \" + str((probs[i]*100).round(4)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients in PyTorch: Autograd\n",
    "Recall that in order to adjust the weights of the model we want to know the direction of the steepest descent. Therefore, we need the gradient, which is a vector of the partial derivatives of the function. The gradient of a function of variables points to the direction of the greatest increase of the objective function. Taking a step towards the direction of greatest descent corresponds to taking a step towards the negative gradient. \n",
    "\n",
    "In PyTorch, `Autograd()` automatically calculates the gradients of tensors. It works by keeping track of operations performed on tensors, then going backwards trhough thse operations and calculating gradients along the way.\n",
    "- * `requires_grad = True` must be set on a tensor.\n",
    "* This can be done at creation of a tensor with `requires_grad`keyword, or at any time with `x.requires_grad_(True).\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`.\n",
    "\n",
    "#### Example of Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.0314, -0.4865],\n",
      "        [ 1.2349, -0.6947]], requires_grad=True)\n",
      "\n",
      "y:\n",
      " tensor([[9.8870e-04, 2.3671e-01],\n",
      "        [1.5249e+00, 4.8260e-01]], grad_fn=<PowBackward0>)\n",
      "\n",
      "grad object of y:\n",
      " <PowBackward0 object at 0x000001D15BA55470>\n",
      "\n",
      "z:\n",
      " tensor(0.5613, grad_fn=<MeanBackward1>)\n",
      "\n",
      "grad of x:\n",
      " tensor([[ 0.0314, -0.4865],\n",
      "        [ 1.2349, -0.6947]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with autograd\n",
    "x = torch.randn(2,2, requires_grad = True)\n",
    "y = x**2\n",
    "z = y.mean()\n",
    "\n",
    "# print\n",
    "print(\"x:\\n\",x)\n",
    "print(\"\\ny:\\n\",y)\n",
    "print(\"\\ngrad object of y:\\n\",y.grad_fn)\n",
    "print(\"\\nz:\\n\",z)\n",
    "print(\"\\ngrad of x:\\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward()...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to the example: Loss and Autograd together\n",
    "When we create a networks with PyTorch, all of the parameters are initialized with `requires_grad = True.`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward:\n",
      " None\n",
      "After backward():\n",
      " tensor([[ 0.0052,  0.0052,  0.0052,  ...,  0.0052,  0.0052,  0.0052],\n",
      "        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0025,  0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002]])\n"
     ]
    }
   ],
   "source": [
    "# Same model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# criterion\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "output_scores = model(images) # Logits\n",
    "loss = criterion(output_scores, labels)\n",
    "\n",
    "# Gradients of weight matrix of first layer\n",
    "print(\"Before backward:\\n\",model[0].weight.grad)\n",
    "loss.backward()\n",
    "print(\"After backward():\\n\",model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "We need one more thing before training the network: An optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's [`optim` package](https://pytorch.org/docs/stable/optim.html). For example we can use stochastic gradient descent with `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen all the individual parts and it is time to bring everything together. At first, we'll consider one learning step before looping through all the data.\n",
    "\n",
    "In general, the steps in PyTorch are:\n",
    "\n",
    "- Make a forward pass through the network: `output = model.forward(images)`\n",
    "- Use the network output to calculate the loss: `loss = criterion(output, labels)`\n",
    "- Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "- Take a step with the optimizer to update the weights: `optimizer.step()`\n",
    "\n",
    "Below we'll go through one training step and print out the weights and gradients:\n",
    "\n",
    "Note that there is a line of code `optimizer.zero_grad()`. Whenever we perform multiple backwards passes with the same parameters, the gradients are accumulated. This means that we need to zero the gradients on each training pass or we'll retain gradients from previous training batches as we'll perform this process iteratively in a for loop.\n",
    "\n",
    "We'll see how this works by taking a look at the first layer of our network. We will focus on the five weights and gradients of the first image. Recall that the weights are updated using the gradient with a learning rate $\\alpha$ (We'll use 0.1):\n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights parameters:\n",
      " tensor([-0.0345, -0.0270, -0.0218, -0.0307, -0.0291], grad_fn=<SliceBackward>)\n",
      "\n",
      "gradients:\n",
      " tensor([5.8640e-05, 5.8640e-05, 5.8640e-05, 5.8640e-05, 5.8640e-05])\n",
      "\n",
      "gradients*learnrate:\n",
      " tensor([5.8640e-06, 5.8640e-06, 5.8640e-06, 5.8640e-06, 5.8640e-06])\n",
      "\n",
      "New weights parameters:\n",
      " tensor([-0.0345, -0.0270, -0.0218, -0.0308, -0.0291], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "# initial weights\n",
    "print(\"Initial weights parameters:\\n\",model[0].weight[0,:5])\n",
    "\n",
    "# data\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# clear gradients \n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward pass\n",
    "output = model.forward(images)\n",
    "\n",
    "# loss\n",
    "loss = criterion(output, labels)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "\n",
    "# gradients\n",
    "print(\"\\ngradients:\\n\", model[0].weight.grad[0, :5])\n",
    "print(\"\\ngradients*learnrate:\\n\",model[0].weight.grad[0, :5]*0.1)\n",
    "\n",
    "# take a step towards negative gradient\n",
    "optimizer.step()\n",
    "\n",
    "# new weights: w = \n",
    "print(\"\\nNew weights parameters:\\n\", model[0].weight[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "In the example above we have looked at only a small part of how the network changes. Note that this happens for the entire model simultaneously.\n",
    "\n",
    "Next step is to iteratively repeat this process using a for loop until we have found the best possible parameters for our network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
