{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning w PyTorch IV (mnist, mlp)\n",
    "- Part I: [Deep Learning w PyTorch I (intro)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20I%20(intro).ipynb)\n",
    "- Part II: [Deep Learning w PyTorch II (define a nn)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20II%20(define%20a%20nn).ipynb)\n",
    "- Part III: [Deep Learning w PyTorch III (training nn, theory)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20III%20(training%20nn%2C%20theory).ipynb)\n",
    "- Part V: [Deep Learning w PyTorch V (fmnist, mlp, inference and validation)](https://github.com/tm1611/Deep-Learning/blob/master/Deep%20Learning%20w%20PyTorch%20V%20(fmnist%2C%20mlp%2C%20inference%20and%20validation).ipynb)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "Our next step is to demonstrate how a neural network can be trained on the [mnist data](https://en.wikipedia.org/wiki/MNIST_database). We will see how to fit a model before moving on to inference, validation and other topics.\n",
    "\n",
    "The objective is to identify handwritten numbers using a multilayer perceptron neural network with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "\n",
    "# visualizations\n",
    "import helper\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data\n",
    "Import the data from `datasets.MNIST()`, define a transformation to normalize the data, use `DataLoader()` for importing a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "\n",
    "# Download Data \n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', \n",
    "                          download=True, \n",
    "                          train=True, \n",
    "                          transform=transform)\n",
    "\n",
    "# Define DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=64, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Model\n",
    "- Define a multilayer perceptron neural network: `model = nn.Sequential(...)` with linear layers (`nn.Linear()`)\n",
    "- Define loss function: `criterion = nn.NLLLoss()` \n",
    "- Define optimizer: `optimizer = optim.SGD(...)`\n",
    "- use a for-loop: `for e in range(epochs): ...` that tracks the `running_loss = 0`\n",
    " - load images `for images, labels in trainloader:`\n",
    " - flatten images`images = images.reshape(images.shape[0], -1)\n",
    " - set gradients to zero: `optimizer.zero_grad()`\n",
    " - forward pass: `output = model.forward(iamges)`\n",
    " - calculate loss: `loss = criterion(output, labels)`\n",
    " - calculate gradients of loss function: `loss.backward()`\n",
    " - take one step towards steepest descent: `optimizer.step()`\n",
    "- Add loss to running_loss: `running_loss += loss.item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Training loss: 0.3433\n",
      "Epoch: 2/20\n",
      "Training loss: 0.189\n",
      "Epoch: 3/20\n",
      "Training loss: 0.1635\n",
      "Epoch: 4/20\n",
      "Training loss: 0.1532\n",
      "Epoch: 5/20\n",
      "Training loss: 0.1321\n",
      "Epoch: 6/20\n",
      "Training loss: 0.1305\n",
      "Epoch: 7/20\n",
      "Training loss: 0.1282\n",
      "Epoch: 8/20\n",
      "Training loss: 0.1189\n",
      "Epoch: 9/20\n",
      "Training loss: 0.1121\n",
      "Epoch: 10/20\n",
      "Training loss: 0.1137\n",
      "Epoch: 11/20\n",
      "Training loss: 0.1104\n",
      "Epoch: 12/20\n",
      "Training loss: 0.1078\n",
      "Epoch: 13/20\n",
      "Training loss: 0.1061\n",
      "Epoch: 14/20\n",
      "Training loss: 0.1046\n",
      "Epoch: 15/20\n",
      "Training loss: 0.0953\n",
      "Epoch: 16/20\n",
      "Training loss: 0.0979\n",
      "Epoch: 17/20\n",
      "Training loss: 0.0992\n",
      "Epoch: 18/20\n",
      "Training loss: 0.0875\n",
      "Epoch: 19/20\n",
      "Training loss: 0.0888\n",
      "Epoch: 20/20\n",
      "Training loss: 0.0915\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 20\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "                      \n",
    "# criterion\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.005)\n",
    "\n",
    "# Gradient descent\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        print(f\"Training loss:\", round(train_loss,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Next, we'll check the predictions of this model. We'll turn off gradients to speed up the process (and we don't need them here anyway). We can do this temporarily as:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logps = model.forward(img)\n",
    "```\n",
    "The combination of `nn.NLLLoss()` (see [documentation](https://pytorch.org/docs/stable/nn.html#nllloss)) which is the negative log likelihood loss with `nn.LogSoftmax(dim=1)` yields log-probabilities. Taking the exponential gives us probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b8c1f127f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load one batch\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Take first image\n",
    "img = images[0].reshape(1, 784)\n",
    "\n",
    "# Turn off gradients\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "    \n",
    "# take exp\n",
    "probs = torch.exp(output)\n",
    "\n",
    "# Visualize results with helper\n",
    "helper.view_classify(img.view(1, 28, 28), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next steps\n",
    "In the next notebook, we'll introduce a related dataset (fashion mnist). After fitting a neural network model for prediction we'll take one step further and use training curves for inference and validating the fit of the model on unseen data calculating the accuracy. Moreover, we'll use dropout to improve the generalizability of the model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
