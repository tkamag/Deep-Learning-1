{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning w PyTorch IV (mnist, mlp)\n",
    "- Part 1: ...\n",
    "\n",
    "## 1. Introduction\n",
    "The following notebook briefly demonstrates how the previously introduced methodology can be applied to the [mnist data](https://en.wikipedia.org/wiki/MNIST_database). It is just to show briefly how to fit a basic model before moving on to inference, validation and other topics.\n",
    "\n",
    "The objective is to identify the handwritten number using a multilayer perceptron neural network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "\n",
    "# visualizations\n",
    "import helper\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data\n",
    "Import the data from `datasets.MNIST()`, define a transformation to normalize the data, use `DataLoader()` for importing a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "\n",
    "# Download Data \n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', \n",
    "                          download=True, \n",
    "                          train=True, \n",
    "                          transform=transform)\n",
    "\n",
    "# Define DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=64, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Model\n",
    "- Define a multilayer perceptron neural network: `model = nn.Sequential(...)`\n",
    "- Define loss function: `criterion = nn.NLLLoss()` \n",
    "- Define optimizer: `optimizer = optim.SGD(...)`\n",
    "- use a for-loop: `for e in range(epochs): ...` that tracks the `running_loss = 0`\n",
    " - load images `for images, labels in trainloader:`\n",
    " - flatten images`images = images.reshape(images.shape[0], -1)\n",
    " - set gradients to zero: `optimizer.zero_grad()`\n",
    " - forward pass: `output = model.forward(iamges)`\n",
    " - calculate loss: `loss = criterion(output, labels)`\n",
    " - calculate gradients of loss function: `loss.backward()`\n",
    " - take one step towards steepest descent: `optimizer.step()`\n",
    "- Add loss to running_loss: `running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Training loss: 0.3436\n",
      "Epoch: 2/20\n",
      "Training loss: 0.1878\n",
      "Epoch: 3/20\n",
      "Training loss: 0.1603\n",
      "Epoch: 4/20\n",
      "Training loss: 0.1434\n",
      "Epoch: 5/20\n",
      "Training loss: 0.1405\n",
      "Epoch: 6/20\n",
      "Training loss: 0.1279\n",
      "Epoch: 7/20\n",
      "Training loss: 0.1248\n",
      "Epoch: 8/20\n",
      "Training loss: 0.1162\n",
      "Epoch: 9/20\n",
      "Training loss: 0.1118\n",
      "Epoch: 10/20\n",
      "Training loss: 0.1116\n",
      "Epoch: 11/20\n",
      "Training loss: 0.1089\n",
      "Epoch: 12/20\n",
      "Training loss: 0.1018\n",
      "Epoch: 13/20\n",
      "Training loss: 0.1015\n",
      "Epoch: 14/20\n",
      "Training loss: 0.0949\n",
      "Epoch: 15/20\n",
      "Training loss: 0.0938\n",
      "Epoch: 16/20\n",
      "Training loss: 0.0907\n",
      "Epoch: 17/20\n",
      "Training loss: 0.0912\n",
      "Epoch: 18/20\n",
      "Training loss: 0.0888\n",
      "Epoch: 19/20\n",
      "Training loss: 0.0879\n",
      "Epoch: 20/20\n",
      "Training loss: 0.0926\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 20\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "                      \n",
    "# criterion\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.005)\n",
    "\n",
    "# Gradient descent\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        print(f\"Training loss:\", round(train_loss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Training loss: 1.4581\n",
      "Epoch: 2/20\n",
      "Training loss: 0.5233\n",
      "Epoch: 3/20\n",
      "Training loss: 0.3928\n",
      "Epoch: 4/20\n",
      "Training loss: 0.3486\n",
      "Epoch: 5/20\n",
      "Training loss: 0.3236\n",
      "Epoch: 6/20\n",
      "Training loss: 0.3059\n",
      "Epoch: 7/20\n",
      "Training loss: 0.2921\n",
      "Epoch: 8/20\n",
      "Training loss: 0.2794\n",
      "Epoch: 9/20\n",
      "Training loss: 0.2688\n",
      "Epoch: 10/20\n",
      "Training loss: 0.2584\n",
      "Epoch: 11/20\n",
      "Training loss: 0.2479\n",
      "Epoch: 12/20\n",
      "Training loss: 0.2383\n",
      "Epoch: 13/20\n",
      "Training loss: 0.2289\n",
      "Epoch: 14/20\n",
      "Training loss: 0.2196\n",
      "Epoch: 15/20\n",
      "Training loss: 0.2109\n",
      "Epoch: 16/20\n",
      "Training loss: 0.2028\n",
      "Epoch: 17/20\n",
      "Training loss: 0.1945\n",
      "Epoch: 18/20\n",
      "Training loss: 0.1868\n",
      "Epoch: 19/20\n",
      "Training loss: 0.1798\n",
      "Epoch: 20/20\n",
      "Training loss: 0.1733\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 20\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "                      \n",
    "# criterion\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.005)\n",
    "\n",
    "# Gradient descent\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        print(f\"Training loss:\", round(train_loss,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Next, we'll check the predictions of this model. We'll turn off gradients to speed this process up and we don't need them here anyway. We can do this temporarily with:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logps = model.forward(img)\n",
    "```\n",
    "The combination of `nn.NLLLoss()` (see [documentation](https://pytorch.org/docs/stable/nn.html#nllloss)) which is the negative log likelihood loss with `nn.LogSoftmax(dim=1)` yields log-probabilities. Taking the exponential gives us probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x193e6f7d358>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load one batch\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Take first image\n",
    "img = images[0].reshape(1, 784)\n",
    "\n",
    "# Turn off gradients\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "    \n",
    "# take exp\n",
    "probs = torch.exp(output)\n",
    "\n",
    "# Visualize results with helper\n",
    "helper.view_classify(img.view(1, 28, 28), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next steps\n",
    "In the next notebook, we'll introduce a related dataset (fashion mnist) and built on what we have done so far. After fitting a neural network model for prediction we'll take one step further and use some methods like training curves for inference and validating the fit of the model on unseen data. Moreover, we'll use dropout to improve the generalizability of the model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
